{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.17\r\n"
     ]
    }
   ],
   "source": [
    "! python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbstractLazySequence', 'AffixTagger', 'AlignedSent', 'Alignment', 'AnnotationTask', 'ApplicationExpression', 'Assignment', 'BigramAssocMeasures', 'BigramCollocationFinder', 'BigramTagger', 'BinaryMaxentFeatureEncoding', 'BlanklineTokenizer', 'BllipParser', 'BottomUpChartParser', 'BottomUpLeftCornerChartParser', 'BottomUpProbabilisticChartParser', 'Boxer', 'BrillTagger', 'BrillTaggerTrainer', 'CFG', 'CRFTagger', 'CfgReadingCommand', 'ChartParser', 'ChunkParserI', 'ChunkScore', 'Cistem', 'ClassifierBasedPOSTagger', 'ClassifierBasedTagger', 'ClassifierI', 'ConcordanceIndex', 'ConditionalExponentialClassifier', 'ConditionalFreqDist', 'ConditionalProbDist', 'ConditionalProbDistI', 'ConfusionMatrix', 'ContextIndex', 'ContextTagger', 'ContingencyMeasures', 'CoreNLPDependencyParser', 'CoreNLPParser', 'Counter', 'CrossValidationProbDist', 'DRS', 'DecisionTreeClassifier', 'DefaultTagger', 'DependencyEvaluator', 'DependencyGrammar', 'DependencyGraph', 'DependencyProduction', 'DictionaryConditionalProbDist', 'DictionaryProbDist', 'DiscourseTester', 'DrtExpression', 'DrtGlueReadingCommand', 'ELEProbDist', 'EarleyChartParser', 'Expression', 'FStructure', 'FeatDict', 'FeatList', 'FeatStruct', 'FeatStructReader', 'Feature', 'FeatureBottomUpChartParser', 'FeatureBottomUpLeftCornerChartParser', 'FeatureChartParser', 'FeatureEarleyChartParser', 'FeatureIncrementalBottomUpChartParser', 'FeatureIncrementalBottomUpLeftCornerChartParser', 'FeatureIncrementalChartParser', 'FeatureIncrementalTopDownChartParser', 'FeatureTopDownChartParser', 'FreqDist', 'HTTPPasswordMgrWithDefaultRealm', 'HeldoutProbDist', 'HiddenMarkovModelTagger', 'HiddenMarkovModelTrainer', 'HunposTagger', 'IBMModel', 'IBMModel1', 'IBMModel2', 'IBMModel3', 'IBMModel4', 'IBMModel5', 'ISRIStemmer', 'ImmutableMultiParentedTree', 'ImmutableParentedTree', 'ImmutableProbabilisticMixIn', 'ImmutableProbabilisticTree', 'ImmutableTree', 'IncrementalBottomUpChartParser', 'IncrementalBottomUpLeftCornerChartParser', 'IncrementalChartParser', 'IncrementalLeftCornerChartParser', 'IncrementalTopDownChartParser', 'Index', 'InsideChartParser', 'JSONTaggedDecoder', 'JSONTaggedEncoder', 'KneserNeyProbDist', 'LancasterStemmer', 'LaplaceProbDist', 'LazyConcatenation', 'LazyEnumerate', 'LazyIteratorList', 'LazyMap', 'LazySubsequence', 'LazyZip', 'LeftCornerChartParser', 'LidstoneProbDist', 'LineTokenizer', 'LogicalExpressionException', 'LongestChartParser', 'MLEProbDist', 'MWETokenizer', 'Mace', 'MaceCommand', 'MaltParser', 'MaxentClassifier', 'Model', 'MultiClassifierI', 'MultiParentedTree', 'MutableProbDist', 'NaiveBayesClassifier', 'NaiveBayesDependencyScorer', 'NgramAssocMeasures', 'NgramTagger', 'NonprojectiveDependencyParser', 'Nonterminal', 'OrderedDict', 'PCFG', 'Paice', 'ParallelProverBuilder', 'ParallelProverBuilderCommand', 'ParentedTree', 'ParserI', 'PerceptronTagger', 'PhraseTable', 'PorterStemmer', 'PositiveNaiveBayesClassifier', 'ProbDistI', 'ProbabilisticDependencyGrammar', 'ProbabilisticMixIn', 'ProbabilisticNonprojectiveParser', 'ProbabilisticProduction', 'ProbabilisticProjectiveDependencyParser', 'ProbabilisticTree', 'Production', 'ProjectiveDependencyParser', 'Prover9', 'Prover9Command', 'ProxyBasicAuthHandler', 'ProxyDigestAuthHandler', 'ProxyHandler', 'PunktSentenceTokenizer', 'QuadgramAssocMeasures', 'QuadgramCollocationFinder', 'RSLPStemmer', 'RTEFeatureExtractor', 'RUS_PICKLE', 'RandomChartParser', 'RangeFeature', 'ReadingCommand', 'RecursiveDescentParser', 'RegexpChunkParser', 'RegexpParser', 'RegexpStemmer', 'RegexpTagger', 'RegexpTokenizer', 'ReppTokenizer', 'ResolutionProver', 'ResolutionProverCommand', 'SExprTokenizer', 'SLASH', 'Senna', 'SennaChunkTagger', 'SennaNERTagger', 'SennaTagger', 'SequentialBackoffTagger', 'ShiftReduceParser', 'SimpleGoodTuringProbDist', 'SklearnClassifier', 'SlashFeature', 'SnowballStemmer', 'SpaceTokenizer', 'StackDecoder', 'StanfordNERTagger', 'StanfordPOSTagger', 'StanfordSegmenter', 'StanfordTagger', 'StemmerI', 'SteppingChartParser', 'SteppingRecursiveDescentParser', 'SteppingShiftReduceParser', 'SyllableTokenizer', 'TYPE', 'TabTokenizer', 'TableauProver', 'TableauProverCommand', 'TaggerI', 'TestGrammar', 'Text', 'TextCat', 'TextCollection', 'TextTilingTokenizer', 'TnT', 'TokenSearcher', 'ToktokTokenizer', 'TopDownChartParser', 'TransitionParser', 'Tree', 'TreebankWordTokenizer', 'Trie', 'TrigramAssocMeasures', 'TrigramCollocationFinder', 'TrigramTagger', 'TweetTokenizer', 'TypedMaxentFeatureEncoding', 'Undefined', 'UniformProbDist', 'UnigramTagger', 'UnsortedChartParser', 'Valuation', 'Variable', 'ViterbiParser', 'WekaClassifier', 'WhitespaceTokenizer', 'WittenBellProbDist', 'WordNetLemmatizer', 'WordPunctTokenizer', '__author__', '__author_email__', '__builtins__', '__cached__', '__classifiers__', '__copyright__', '__doc__', '__file__', '__keywords__', '__license__', '__loader__', '__longdescr__', '__maintainer__', '__maintainer_email__', '__name__', '__package__', '__path__', '__spec__', '__url__', '__version__', 'absolute_import', 'accuracy', 'add_logs', 'agreement', 'align', 'alignment_error_rate', 'aline', 'api', 'app', 'apply_features', 'approxrand', 'arity', 'association', 'bigrams', 'binary_distance', 'binary_search_file', 'binding_ops', 'bisect', 'blankline_tokenize', 'bleu', 'bleu_score', 'bllip', 'boolean_ops', 'boxer', 'bracket_parse', 'breadth_first', 'brill', 'brill_trainer', 'build_opener', 'call_megam', 'casual', 'casual_tokenize', 'ccg', 'chain', 'chart', 'chat', 'choose', 'chunk', 'cistem', 'class_types', 'classify', 'clause', 'clean_html', 'clean_url', 'cluster', 'collections', 'collocations', 'combinations', 'compat', 'config_java', 'config_megam', 'config_weka', 'conflicts', 'confusionmatrix', 'conllstr2tree', 'conlltags2tree', 'corenlp', 'corpus', 'crf', 'custom_distance', 'data', 'decisiontree', 'decorator', 'decorators', 'defaultdict', 'demo', 'dependencygraph', 'deque', 'discourse', 'distance', 'download', 'download_gui', 'download_shell', 'downloader', 'draw', 'drt', 'earleychart', 'edit_distance', 'edit_distance_align', 'elementtree_indent', 'entropy', 'equality_preds', 'evaluate', 'evaluate_sents', 'everygrams', 'extract_rels', 'extract_test_sentences', 'f_measure', 'featstruct', 'featurechart', 'filestring', 'find', 'flatten', 'fractional_presence', 'getproxies', 'ghd', 'glue', 'grammar', 'guess_encoding', 'help', 'hmm', 'hunpos', 'ibm1', 'ibm2', 'ibm3', 'ibm4', 'ibm5', 'ibm_model', 'ieerstr2tree', 'improved_close_quote_regex', 'improved_open_quote_regex', 'improved_open_single_quote_regex', 'improved_punct_regex', 'in_idle', 'induce_pcfg', 'inference', 'infile', 'inspect', 'install_opener', 'internals', 'interpret_sents', 'interval_distance', 'invert_dict', 'invert_graph', 'is_rel', 'islice', 'isri', 'jaccard_distance', 'json_tags', 'jsontags', 'lancaster', 'lazyimport', 'lfg', 'line_tokenize', 'linearlogic', 'lm', 'load', 'load_parser', 'locale', 'log_likelihood', 'logic', 'mace', 'malt', 'map_tag', 'mapping', 'masi_distance', 'maxent', 'megam', 'memoize', 'meteor', 'meteor_score', 'metrics', 'misc', 'mwe', 'naivebayes', 'ne_chunk', 'ne_chunk_sents', 'ngrams', 'nonprojectivedependencyparser', 'nonterminals', 'numpy', 'os', 'pad_sequence', 'paice', 'parse', 'parse_sents', 'pchart', 'perceptron', 'pk', 'porter', 'pos_tag', 'pos_tag_sents', 'positivenaivebayes', 'pprint', 'pr', 'precision', 'presence', 'print_function', 'print_string', 'probability', 'projectivedependencyparser', 'prover9', 'punkt', 'py25', 'py26', 'py27', 'pydoc', 'python_2_unicode_compatible', 'raise_unorderable_types', 'ranks_from_scores', 'ranks_from_sequence', 're', 're_show', 'read_grammar', 'read_logic', 'read_valuation', 'recall', 'recursivedescent', 'regexp', 'regexp_span_tokenize', 'regexp_tokenize', 'register_tag', 'relextract', 'repp', 'resolution', 'ribes', 'ribes_score', 'root_semrep', 'rslp', 'rte_classifier', 'rte_classify', 'rte_features', 'rtuple', 'scikitlearn', 'scores', 'segmentation', 'sem', 'senna', 'sent_tokenize', 'sequential', 'set2rel', 'set_proxy', 'sexpr', 'sexpr_tokenize', 'shiftreduce', 'simple', 'sinica_parse', 'skipgrams', 'skolemize', 'slice_bounds', 'snowball', 'sonority_sequencing', 'spearman', 'spearman_correlation', 'stack_decoder', 'stanford', 'stanford_segmenter', 'stem', 'str2tuple', 'string_span_tokenize', 'string_types', 'subprocess', 'subsumes', 'sum_logs', 'sys', 'tableau', 'tadm', 'tag', 'tagset_mapping', 'tagstr2tree', 'tbl', 'text', 'text_type', 'textcat', 'texttiling', 'textwrap', 'tkinter', 'tnt', 'tokenize', 'tokenwrap', 'toktok', 'toolbox', 'total_ordering', 'transitionparser', 'transitive_closure', 'translate', 'tree', 'tree2conllstr', 'tree2conlltags', 'treebank', 'treetransforms', 'trigrams', 'tuple2str', 'types', 'unify', 'unique_list', 'untag', 'usage', 'util', 'version_file', 'version_info', 'viterbi', 'weka', 'windowdiff', 'word_tokenize', 'wordnet', 'wordpunct_tokenize', 'wsd']\n"
     ]
    }
   ],
   "source": [
    "print(dir(nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "Error connecting to server: [Errno -2] Name or service not known\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [*] abc................. Australian Broadcasting Commission 2006\n",
      "  [*] alpino.............. Alpino Dutch Treebank\n",
      "  [*] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [*] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [*] basque_grammars..... Grammars for Basque\n",
      "  [*] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [*] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [*] book_grammars....... Grammars from NLTK Book\n",
      "  [*] brown............... Brown Corpus\n",
      "  [*] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [*] cess_cat............ CESS-CAT Treebank\n",
      "  [*] cess_esp............ CESS-ESP Treebank\n",
      "  [*] chat80.............. Chat-80 Data Files\n",
      "  [*] city_database....... City Database\n",
      "  [*] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [*] comparative_sentences Comparative Sentence Dataset\n",
      "  [*] comtrans............ ComTrans Corpus Sample\n",
      "  [*] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [*] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: q\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> Quit\n",
      "Command 'Quit' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/rahmanuddin/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma = nltk.corpus.gutenberg.raw('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n"
     ]
    }
   ],
   "source": [
    "print(dir(emma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887120\n"
     ]
    }
   ],
   "source": [
    "print(emma.__sizeof__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; an\n"
     ]
    }
   ],
   "source": [
    "print(emma[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Woodhouse,',\n",
       " 'handsome,',\n",
       " 'clever,',\n",
       " 'and',\n",
       " 'rich,',\n",
       " 'with',\n",
       " 'a',\n",
       " 'comfortable',\n",
       " 'home',\n",
       " 'and']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma.split()[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
